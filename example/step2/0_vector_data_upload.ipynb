{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 2-0. 로컬 임베딩 모델을 사용한 벡터 데이터 업로드\nSentence Transformers 모델로 텍스트를 벡터로 변환하여 OpenSearch에 업로드합니다.\n\n## 배경 지식\n\n### 벡터(Vector)와 임베딩(Embedding)\n- **벡터**: 숫자의 목록(리스트). 어떤 개념이나 의미를 수치로 표현한 것\n- **word2vec (= embedding)**: 단어가 다르더라도 **의미가 비슷한 것**을 찾을 수 있음\n  - 예: \"사람\" [0, 0, 1] vs \"그러고도 니가 사람이야?\" [0, 0, 0.1] → 벡터 방향이 같으면 의미가 비슷\n  - 반면: \"사람\" [0, 0, 1] vs \"바나나\" [1, 0, 0] → 벡터 방향이 다르면 의미도 다름\n\n### Vector Search\n텍스트나 이미지 등을 벡터로 변환한 후, 유사한 벡터를 검색하여 **의미 기반**으로 결과를 찾는 기술\n- **코사인 유사도**: 벡터 간 각도를 이용해 유사도 측정 (방향이 같으면 유사)\n- **유클리드 거리**: 벡터 간 거리로 유사도 측정\n- BM25(키워드 기반)와의 차이: Vector Search는 **\"의미 기반 검색\"**\n\n### k-NN (k-Nearest Neighbors)\n새로운 데이터가 주어졌을 때, 주변에 있는 **k개의 이웃**을 보고 판단하는 알고리즘\n- **k**: 주변 이웃 개수 (작으면 민감, 크면 덜 민감)\n- **distance metric**: 거리 계산 방법 (유클리드, 코사인 등)"
  },
  {
   "cell_type": "code",
   "source": "!pip install -q boto3==1.38.46 opensearch-py==2.8.0 sentence-transformers==4.1.0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:43:00.652665Z",
     "start_time": "2026-02-12T07:42:59.860763Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 설정 (Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:43:02.043082Z",
     "start_time": "2026-02-12T07:43:02.039468Z"
    }
   },
   "source": [
    "import os, json\n",
    "\n",
    "# Step 0에서 저장한 설정 불러오기\n",
    "try:\n",
    "    with open(\"../config.json\") as f:\n",
    "        _config = json.load(f)\n",
    "    print(\"✅ config.json 로드 완료\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"❌ config.json을 찾을 수 없습니다. Step 0 노트북을 먼저 실행해주세요.\")\n",
    "\n",
    "HOST = _config.get(\"OPENSEARCH_HOST\")\n",
    "if not HOST:\n",
    "    raise ValueError(\"❌ config.json에 OPENSEARCH_HOST 값이 없습니다. Step 0 노트북을 먼저 실행해주세요.\")\n",
    "DEFAULT_REGION = _config.get(\"DEFAULT_REGION\", \"ap-northeast-2\")\n",
    "PROFILE = _config.get(\"PROFILE\", \"skku-opensearch-session\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenSearch 클라이언트 생성"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:43:41.993157Z",
     "start_time": "2026-02-12T07:43:41.982112Z"
    }
   },
   "source": [
    "import boto3\n",
    "from opensearchpy import OpenSearch, AWSV4SignerAuth, RequestsHttpConnection\n",
    "\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session(profile_name=PROFILE).get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, DEFAULT_REGION, service)\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts=[{'host': HOST, 'port': 443}],\n",
    "    http_auth=auth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "\n",
    "print(\"OpenSearch 클라이언트 생성 완료\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenSearch 클라이언트 생성 완료\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 임베딩 모델 로드\n",
    "한국어 포함 다국어에 성능이 좋은 Sentence Transformer 모델을 로드합니다.\n",
    "처음 실행 시 모델 다운로드가 필요하여 시간이 걸릴 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:44:48.488126Z",
     "start_time": "2026-02-12T07:43:44.295071Z"
    }
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "vector_dimension = 384  # 이 모델의 벡터 차원 수\n",
    "\n",
    "print(f\"Loading embedding model '{embedding_model_name}'...\")\n",
    "model = SentenceTransformer(embedding_model_name)\n",
    "print(\"Embedding model loaded successfully.\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gnidoc327/ws/side/skku-kdt-opensearch-25/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'...\n",
      "Embedding model loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 벡터 인덱스 생성\n",
    "k-NN 벡터 검색을 위한 인덱스 설정 및 매핑을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:44:54.065644Z",
     "start_time": "2026-02-12T07:44:48.508896Z"
    }
   },
   "source": [
    "from time import sleep\n",
    "\n",
    "index_name = 'vector-test'\n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True,\n",
    "            \"knn.algo_param.ef_search\": 100,\n",
    "            \"analysis\": {\n",
    "                \"analyzer\": {\n",
    "                    \"korean_nori_analyzer\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"nori_tokenizer\",\n",
    "                        \"filter\": [\n",
    "                            \"nori_part_of_speech\",\n",
    "                            \"nori_readingform\",\n",
    "                            \"lowercase\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"content_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": vector_dimension,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"cosinesimil\",\n",
    "                    \"engine\": \"nmslib\"\n",
    "                }\n",
    "            },\n",
    "            \"post_id\": {\"type\": \"integer\"},\n",
    "            \"title\": {\"type\": \"text\", \"analyzer\": \"korean_nori_analyzer\"},\n",
    "            \"content\": {\"type\": \"text\", \"analyzer\": \"korean_nori_analyzer\"},\n",
    "            \"author\": {\"type\": \"keyword\"},\n",
    "            \"category\": {\"type\": \"keyword\"},\n",
    "            \"tags\": {\"type\": \"keyword\"},\n",
    "            \"created_at\": {\"type\": \"date\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if not client.indices.exists(index=index_name):\n",
    "    print(f\"Creating new index '{index_name}'...\")\n",
    "    response = client.indices.create(index=index_name, body=index_body)\n",
    "    print(f\"Index '{index_name}' created successfully.\")\n",
    "    sleep(5)\n",
    "else:\n",
    "    print(f\"Index '{index_name}' already exists.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new index 'vector-test'...\n",
      "Index 'vector-test' created successfully.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 데이터 임베딩 및 업로드\n",
    "JSON 데이터를 로드하고, 각 문서의 제목+내용을 벡터로 변환하여 Bulk API로 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-12T07:45:02.935959Z",
     "start_time": "2026-02-12T07:44:57.016382Z"
    }
   },
   "source": "import json\nfrom opensearchpy import helpers\n\njson_file_path = '../../data/json_data.json'\n\nwith open(json_file_path, 'r', encoding='utf-8') as f:\n    documents = json.load(f)\nprint(f\"Successfully loaded {len(documents)} documents.\")\n\ndef generate_bulk_actions(docs, idx_name):\n    for doc in docs:\n        text_to_embed = f\"{doc.get('title', '')}\\n{doc.get('content', '')}\"\n        vector = model.encode(text_to_embed).tolist()\n\n        source_data = {\n            \"content_vector\": vector,\n            \"post_id\": doc.get(\"post_id\"),\n            \"title\": doc.get(\"title\"),\n            \"content\": doc.get(\"content\"),\n            \"author\": doc.get(\"author\"),\n            \"category\": doc.get(\"category\"),\n            \"tags\": doc.get(\"tags\"),\n            \"created_at\": doc.get(\"created_at\")\n        }\n\n        yield {\n            \"_index\": idx_name,\n            \"_source\": source_data\n        }\n\nprint(\"Starting data embedding and uploading...\")\nsuccess, failed = helpers.bulk(client, generate_bulk_actions(documents, index_name))\n\nprint(f\"Successfully indexed {success} documents.\")\nif failed:\n    print(f\"Failed to index {len(failed)} documents.\")\n    for i, item in enumerate(failed[:5]):\n        print(f\"Failed item {i+1}: {item}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 50 documents.\n",
      "Starting data embedding and uploading...\n",
      "Successfully indexed 50 documents.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}