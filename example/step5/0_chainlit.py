# =============================================================================
# Step 5-0. AI Agent ì±—ë´‡ (Chainlit + MCP)
# íŒ¨í‚¤ì§€ ì„¤ì¹˜: pip install boto3==1.38.46 opensearch-py==2.8.0 chainlit==2.9.6 \
#              langchain-mcp-adapters==0.2.1 langchain-aws==1.2.3 langgraph==1.0.8 \
#              ddgs==9.10.0
# ì‹¤í–‰: cd example/step5 && chainlit run 0_chainlit.py -w
#
# [ì‹¤ìŠµ ê³¼ì œ]
# 1. ë‹¤ì–‘í•œ ì§ˆë¬¸ìœ¼ë¡œ ì—ì´ì „íŠ¸ê°€ ì–´ë–¤ ë„êµ¬ë¥¼ ì„ íƒí•˜ëŠ”ì§€ ê´€ì°°í•´ë³´ì„¸ìš”
#    - "Dockerì™€ Kubernetes ì°¨ì´ì  ì•Œë ¤ì¤˜" â†’ search_documents
#    - "ê°•ì•„ì§€ ì´ë¯¸ì§€ ì°¾ì•„ì¤˜" â†’ search_images
#    - "ì˜¤ëŠ˜ ë‚ ì”¨ ì•Œë ¤ì¤˜" â†’ web_search
# 2. search_mcp_server.pyì— ìƒˆë¡œìš´ ë„êµ¬ë¥¼ ì¶”ê°€í•´ë³´ì„¸ìš”
# =============================================================================
import os
import sys
import json
import warnings

import chainlit as cl
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent
from langchain_aws import ChatBedrockConverse

warnings.filterwarnings("ignore", category=DeprecationWarning)

# --- 1. í™˜ê²½ ì„¤ì • ---
_config_dir = os.path.dirname(os.path.abspath(__file__))
_config_path = os.path.join(_config_dir, "..", "config.json")
with open(_config_path) as _f:
    _config = json.load(_f)

OPENSEARCH_HOST = _config["OPENSEARCH_HOST"]
DEFAULT_REGION = _config.get("DEFAULT_REGION", "ap-northeast-2")
BEDROCK_REGION = _config.get("BEDROCK_REGION", "us-east-1")
AWS_PROFILE = _config.get("PROFILE", "skku-opensearch-session")
LLM_MODEL_ID = "us.anthropic.claude-sonnet-4-5-20250929-v1:0"

TOOL_LABELS = {
    "search_documents": "ğŸ“„ ë¬¸ì„œ ê²€ìƒ‰",
    "search_images": "ğŸ–¼ï¸ ì´ë¯¸ì§€ ê²€ìƒ‰",
    "web_search": "ğŸŒ ì›¹ ê²€ìƒ‰",
}

# --- 2. LLM + MCP í´ë¼ì´ì–¸íŠ¸ ---
model = ChatBedrockConverse(
    model_id=LLM_MODEL_ID,
    region_name=BEDROCK_REGION,
    credentials_profile_name=AWS_PROFILE,
)

server_env = os.environ.copy()
server_env.update({
    "OPENSEARCH_HOST": OPENSEARCH_HOST,
    "DEFAULT_REGION": DEFAULT_REGION,
    "BEDROCK_REGION": BEDROCK_REGION,
    "AWS_PROFILE": AWS_PROFILE,
})

mcp_client = MultiServerMCPClient({
    "search": {
        "transport": "stdio",
        "command": sys.executable,
        "args": [os.path.join(_config_dir, "search_mcp_server.py")],
        "env": server_env,
    }
})

_agent = None


async def get_agent():
    global _agent
    if _agent is None:
        tools = await mcp_client.get_tools()
        _agent = create_react_agent(model, tools)
    return _agent


# --- 3. ë„êµ¬ ê²°ê³¼ â†’ ì°¸ê³ ìë£Œ í¬ë§·íŒ… ---

def _extract_tool_output(event_data):
    """on_tool_end ì´ë²¤íŠ¸ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    output = event_data.get("output", "")
    if hasattr(output, "content"):
        content = output.content
        if isinstance(content, list):
            return "\n".join(
                item.get("text", str(item)) if isinstance(item, dict) else str(item)
                for item in content
            )
        if isinstance(content, str):
            return content
    return str(output)


def _format_web_search_refs(output):
    """ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë§í¬ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    refs = ""
    for group in output.split("\n\n"):
        lines = group.strip().split("\n")
        if len(lines) < 2:
            continue
        title = lines[0].lstrip("[0123456789] ")
        url = next((l[5:] for l in lines if l.startswith("URL: ")), "")
        refs += f"\n- [{title}]({url})" if url else f"\n- {title}"
    return refs


def _format_document_refs(output):
    """ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œëª©+ë¯¸ë¦¬ë³´ê¸°ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    refs = ""
    for group in output.split("\n\n"):
        lines = group.strip().split("\n")
        if not lines:
            continue
        title_line = lines[0]
        title = title_line.split(") ", 1)[1] if ") " in title_line else title_line
        preview = next((l[4:][:80] + "..." for l in lines if l.startswith("ë‚´ìš©: ")), "")
        refs += f"\n- **{title}**"
        if preview:
            refs += f"\n  > {preview}"
    return refs


def _format_image_refs(output):
    """ì´ë¯¸ì§€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¸ë¼ì¸ ì´ë¯¸ì§€ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤."""
    refs = ""
    images = []
    idx = 0
    for line in output.split("\n"):
        if "ì´ë¯¸ì§€ ê²½ë¡œ:" not in line:
            continue
        image_path = line.split("ì´ë¯¸ì§€ ê²½ë¡œ: ")[-1].strip()
        abs_path = image_path if os.path.isabs(image_path) else os.path.normpath(os.path.join(_config_dir, image_path))
        if not os.path.isfile(abs_path):
            refs += f"\n- `{rel_path}` (íŒŒì¼ ì—†ìŒ)"
            continue
        idx += 1
        images.append(cl.Image(name=f"search_result_{idx}", path=abs_path, display="inline"))
        score = ""
        if "ìœ ì‚¬ë„:" in line:
            score = f" (ìœ ì‚¬ë„: {line.split('ìœ ì‚¬ë„: ')[1].split(')')[0]})"
        refs += f"\n- **ì´ë¯¸ì§€ {idx}**{score}\n"
    return refs, images


_REF_FORMATTERS = {
    "web_search": lambda output: (_format_web_search_refs(output), []),
    "search_documents": lambda output: (_format_document_refs(output), []),
    "search_images": _format_image_refs,
}


def build_references(tool_results):
    """ë„êµ¬ ê²°ê³¼ ëª©ë¡ì„ ì°¸ê³ ìë£Œ ë§ˆí¬ë‹¤ìš´ + ì´ë¯¸ì§€ ìš”ì†Œë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if not tool_results:
        return "", []
    refs = "\n\n---\n**ğŸ“š ì°¸ê³  ìë£Œ**\n"
    all_images = []
    for tr in tool_results:
        formatter = _REF_FORMATTERS.get(tr["name"])
        if formatter:
            text, images = formatter(tr["output"])
            refs += text
            all_images.extend(images)
    return refs, all_images


# --- 4. Chainlit ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---

@cl.on_chat_start
async def start():
    await get_agent()
    await cl.Message(
        content="ì•ˆë…•í•˜ì„¸ìš”! OpenSearchì™€ ì›¹ ê²€ìƒ‰ì„ í™œìš©í•œ AI ì±—ë´‡ì…ë‹ˆë‹¤.\n"
                "ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!\n\n"
                "ì˜ˆì‹œ:\n"
                "- `s3ë‘ ê´€ë ¨ìˆëŠ” ê¸€ ì°¾ì•„ì„œ ìš”ì•½í•´ì¤˜` (ë¬¸ì„œ ê²€ìƒ‰)\n"
                "- `ê°•ì•„ì§€ ì´ë¯¸ì§€ ì°¾ì•„ì¤˜` (ì´ë¯¸ì§€ ê²€ìƒ‰)\n"
                "- `2026ë…„ ìµœì‹  AI íŠ¸ë Œë“œ ì•Œë ¤ì¤˜` (ì›¹ ê²€ìƒ‰)"
    ).send()


@cl.on_message
async def main(message: cl.Message):
    agent = await get_agent()
    msg = cl.Message(content="")
    tool_results = []
    current_step = None

    async for event in agent.astream_events(
        {"messages": [("user", message.content)]},
        version="v2",
    ):
        kind = event["event"]

        if kind == "on_tool_start":
            label = TOOL_LABELS.get(event["name"], f"ğŸ”§ {event['name']}")
            current_step = cl.Step(name=label, type="tool")
            current_step.input = event["data"].get("input", {}).get("query", "")
            await current_step.__aenter__()

        elif kind == "on_tool_end":
            output_text = _extract_tool_output(event["data"])
            tool_results.append({"name": event["name"], "output": output_text})
            if current_step:
                current_step.output = output_text[:500] + ("..." if len(output_text) > 500 else "")
                await current_step.__aexit__(None, None, None)
                current_step = None

        elif kind == "on_chat_model_stream":
            chunk = event["data"]["chunk"]
            text = ""
            if isinstance(chunk.content, str):
                text = chunk.content
            elif isinstance(chunk.content, list):
                text = "".join(
                    b.get("text", "") for b in chunk.content
                    if isinstance(b, dict) and b.get("type") == "text"
                )
            if text:
                await msg.stream_token(text)

    refs, images = build_references(tool_results)
    if refs:
        msg.content += refs
    if images:
        msg.elements = images
    await msg.send()
