# streamlit run example/python/step5/0_streamlit.py
# s3ë‘ ê´€ë ¨ìˆëŠ” ê¸€ ì°¾ì•„ì„œ ìš”ì•½í•´ì¤˜
import os
import streamlit as st
import json
import boto3
import sys

# --- í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€ ---
# ì´ ì½”ë“œëŠ” ìŠ¤í¬ë¦½íŠ¸ê°€ ì–´ë””ì„œ ì‹¤í–‰ë˜ë“ , í”„ë¡œì íŠ¸ì˜ ìµœìƒìœ„ ë””ë ‰í† ë¦¬ë¥¼
# íŒŒì´ì¬ ëª¨ë“ˆ ê²€ìƒ‰ ê²½ë¡œì— í¬í•¨ì‹œì¼œ 'from common import ...' êµ¬ë¬¸ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.
# í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ
current_file_path = os.path.abspath(__file__)
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ (í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ì—ì„œ ì„¸ ë‹¨ê³„ ìœ„)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_file_path))))
# sys.pathì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# ---------------------------------------------
from example.python.common.opensearch import client
from example.python.common import config


# --- 1. í™˜ê²½ ì„¤ì • ë° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
session = boto3.Session(profile_name=config.PROFILE)
bedrock = session.client(
    service_name='bedrock-runtime',
    region_name=config.BEDROCK_REGION,
)

# í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸°
try:
    # ìƒìˆ˜ ì •ì˜
    BEDROCK_EMBED_MODEL_ID = 'amazon.titan-embed-text-v2:0'
    BEDROCK_LLM_MODEL_ID = 'us.anthropic.claude-sonnet-4-20250514-v1:0'
    OPENSEARCH_INDEX_NAME = 'bedrock-test' # OpenSearch ì¸ë±ìŠ¤ ì´ë¦„

except Exception as e:
    st.error(f"í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.info("OpenSearch ë° AWS ìê²©ì¦ëª… ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()


# --- 2. ì‹¤ì œ ì—°ë™ í•¨ìˆ˜ ì •ì˜ ---

def get_embedding_from_titan(text: str):
    """
    Bedrock Titan ì„ë² ë”© ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    body = json.dumps({"inputText": text})
    response = bedrock.invoke_model(
        body=body,
        modelId=BEDROCK_EMBED_MODEL_ID,
        accept='application/json',
        contentType='application/json'
    )
    response_body = json.loads(response['body'].read())
    return response_body['embedding']

def search_opensearch_vector(query: str, top_k: int = 3):
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  OpenSearchì—ì„œ k-NN ë²¡í„° ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    print(f"ì‹¤ì œ ë²¡í„° ê²€ìƒ‰ ì‹œì‘: '{query}'")

    # 1. ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
    query_vector = get_embedding_from_titan(query)

    # 2. OpenSearch k-NN ì¿¼ë¦¬ êµ¬ì„±
    search_query = {
        "size": top_k,
        "query": {
            "knn": {
                "content_vector": {
                    "vector": query_vector,
                    "k": top_k
                }
            }
        },
        "_source": ["title", "content", "author", "post_id"]
    }

    # 3. OpenSearchì— ê²€ìƒ‰ ìš”ì²­
    response = client.search(
        index=OPENSEARCH_INDEX_NAME,
        body=search_query
    )

    # 4. ê²°ê³¼ì—ì„œ ë¬¸ì„œ(_source)ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
    retrieved_docs = [hit['_source'] for hit in response['hits']['hits']]
    print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ: {[doc.get('title', 'ì œëª© ì—†ìŒ') for doc in retrieved_docs]}")
    return retrieved_docs

def generate_answer_with_bedrock(context_docs: list, question: str):
    """
    Bedrock Claude 3 Sonnet ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("ì‹¤ì œ Bedrock LLM ë‹µë³€ ìƒì„± ì‹œì‘...")

    if not context_docs:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ ì£¼ì„¸ìš”."

    # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§: ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ì¡°í•©
    context_str = "\n\n".join([
        f"ë¬¸ì„œ ì œëª©: {doc.get('title', 'N/A')}\në‚´ìš©: {doc.get('content', 'N/A')}" for doc in context_docs
    ])

    prompt = f"""ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ì˜ 'ì •ë³´'ë¥¼ ì°¸ê³ í•˜ì—¬ 'ì§ˆë¬¸'ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”. ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.

[ì •ë³´]
{context_str}

[ì§ˆë¬¸]
{question}
"""

    # Bedrock Claude 3 Sonnet í˜¸ì¶œ í˜•ì‹
    conversation = [
        {
            "role": "user",
            "content": [{"type": "text", "text": prompt}],
        }
    ]

    response = bedrock.invoke_model(
        modelId=BEDROCK_LLM_MODEL_ID,
        body=json.dumps(
            {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1024,
                "messages": conversation,
            }
        ),
    )

    response_body = json.loads(response["body"].read())
    answer = response_body["content"][0]["text"]

    return answer


# --- 3. Streamlit UI êµ¬ì„± (ê¸°ì¡´ê³¼ ê±°ì˜ ë™ì¼) ---

st.set_page_config(page_title="RAG ì±—ë´‡ ë°ëª¨", page_icon="ğŸ¤–")

st.title("ğŸ“¦ JSON ë°ì´í„° ê¸°ë°˜ Q&A ì±—ë´‡ (RAG ë°ëª¨)")
st.write("OpenSearchì™€ Bedrockì— ì§ì ‘ ì—°ë™í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")

# ì„¸ì…˜ ìƒíƒœë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ê¸°ë¡ ì €ì¥
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "context" in message and message["context"]:
            with st.expander("ğŸ” ì°¸ê³ í•œ ì •ë³´ ë³´ê¸°"):
                st.json(message["context"])

# ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
if user_question := st.chat_input("ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”! (ì˜ˆ: ëª¨ë‹ˆí„° ì¶©ì „ ê¸°ëŠ¥ ì•Œë ¤ì¤˜)"):
    # ì‚¬ìš©ì ì§ˆë¬¸ì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€í•˜ê³  í™”ë©´ì— í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": user_question})
    with st.chat_message("user"):
        st.markdown(user_question)

    # AI ë‹µë³€ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ì •ë³´ë¥¼ ì°¾ê³  ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            # 1. [Retrieval] ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ (ì‹¤ì œ OpenSearch í˜¸ì¶œ)
            retrieved_docs = search_opensearch_vector(user_question)

            # 2. [Generation] LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ì‹¤ì œ Bedrock í˜¸ì¶œ)
            answer = generate_answer_with_bedrock(retrieved_docs, user_question)

            # 3. ê²°ê³¼ ì¶œë ¥
            st.markdown(answer)

    # AI ë‹µë³€ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({
        "role": "assistant",
        "content": answer,
        "context": retrieved_docs # ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ì„œë¥¼ í•¨ê»˜ ì €ì¥
    })